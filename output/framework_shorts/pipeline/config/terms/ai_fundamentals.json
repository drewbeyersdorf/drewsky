{
  "category": "ai_fundamentals",
  "category_display": "AI Fundamentals",
  "description": "Essential AI and Machine Learning concepts everyone should understand",
  "terms": [
    {
      "id": "training_data",
      "name": "Training Data",
      "complexity": "simple",
      "duration": 20,
      "definition": "Examples used to teach AI systems patterns",
      "hook": "AI doesn't learn like humans. It learns from examples. Lots of examples.",
      "explanation": "Training data is the collection of examples you feed an AI to teach it patterns. Want AI to recognize cats? Show it 10,000 cat photos.",
      "example": "Bad training data? Bad AI. If you only show it orange cats, it won't recognize black cats. Garbage in, garbage out.",
      "why_matters": "The quality and diversity of training data determines how well AI works",
      "on_screen_text": "Training Data = Examples used to teach AI patterns",
      "analogies": ["teaching with flashcards", "learning from textbook examples"],
      "related_terms": ["overfitting", "bias_in_ai"],
      "visual_style": "process",
      "ascii_scenes": ["examples_in", "ai_learning", "pattern_out"]
    },
    {
      "id": "hallucination",
      "name": "Hallucination",
      "complexity": "medium",
      "duration": 25,
      "definition": "When AI confidently generates false information",
      "hook": "AI just made up a fact. With complete confidence. That's a hallucination.",
      "explanation": "Large language models predict what words should come next. Sometimes they predict convincing-sounding nonsense. They don't know they're wrong.",
      "example": "Ask ChatGPT for a book citation? It might invent a book title, author, and ISBN that sound real but don't exist.",
      "why_matters": "Never trust AI output without verification. It can be confidently wrong",
      "on_screen_text": "Hallucination = AI confidently generating false information",
      "analogies": ["confident liar", "dreaming while awake"],
      "related_terms": ["llm", "temperature"],
      "visual_style": "problem_demonstration",
      "ascii_scenes": ["ai_generating", "confident_output", "actually_false"]
    },
    {
      "id": "tokens",
      "name": "Tokens",
      "complexity": "medium",
      "duration": 25,
      "definition": "Chunks of text AI processes (not always full words)",
      "hook": "AI doesn't read words like you do. It reads tokens. What's a token?",
      "explanation": "AI breaks text into chunks called tokens. Common words = 1 token. Rare words might be 2-3 tokens. 'ChatGPT' is 2 tokens: 'Chat' + 'GPT'.",
      "example": "Token limits matter. GPT-4 has a 8k token limit. That's roughly 6,000 words. Hit the limit? AI forgets earlier conversation.",
      "why_matters": "Understanding tokens explains AI costs, limits, and behavior",
      "on_screen_text": "Tokens = Text chunks AI processes (not always words)",
      "analogies": ["syllables in speech", "bytes in files"],
      "related_terms": ["context_window", "llm"],
      "visual_style": "demonstration",
      "ascii_scenes": ["text_input", "tokenization", "chunks"]
    },
    {
      "id": "context_window",
      "name": "Context Window",
      "complexity": "medium",
      "duration": 25,
      "definition": "How much text AI can remember at once",
      "hook": "AI has short-term memory. A really specific short-term memory limit.",
      "explanation": "The context window is how many tokens AI can process at once. It includes your prompt, the conversation history, and the response.",
      "example": "8k context window = roughly 6,000 words total. Long conversation? Old messages get forgotten to make room for new ones.",
      "why_matters": "Explains why AI sometimes loses track of earlier conversation points",
      "on_screen_text": "Context Window = How much text AI remembers at once",
      "analogies": ["short-term memory", "RAM in computers"],
      "related_terms": ["tokens", "llm"],
      "visual_style": "demonstration",
      "ascii_scenes": ["memory_window", "text_sliding", "forgotten_text"]
    },
    {
      "id": "overfitting",
      "name": "Overfitting",
      "complexity": "medium",
      "duration": 25,
      "definition": "AI memorizes training examples instead of learning patterns",
      "hook": "Your AI aced the practice test but failed the real exam. That's overfitting.",
      "explanation": "Overfitting happens when AI memorizes training data instead of learning general patterns. It's like a student who memorizes answers without understanding concepts.",
      "example": "Train AI on 100 cat photos, it memorizes those 100. Show it a new cat? Doesn't recognize it. It memorized instead of learning 'catness'.",
      "why_matters": "Overfitted models fail on real-world data they haven't seen before",
      "on_screen_text": "Overfitting = Memorizing examples instead of learning patterns",
      "analogies": ["memorizing vs understanding", "teaching to the test"],
      "related_terms": ["training_data", "generalization"],
      "visual_style": "problem_demonstration",
      "ascii_scenes": ["perfect_on_training", "fails_on_new", "overfit"]
    },
    {
      "id": "bias_in_ai",
      "name": "Bias in AI",
      "complexity": "medium",
      "duration": 25,
      "definition": "AI inherits biases from training data",
      "hook": "Your AI is racist. Not because it chose to be. Because its training data was.",
      "explanation": "AI learns patterns from data. If training data reflects human biases - racism, sexism, stereotypes - AI learns those too.",
      "example": "Amazon's hiring AI rejected women. Why? Trained on 10 years of resumes from a male-dominated industry. It learned: male = good candidate.",
      "why_matters": "AI can amplify and automate human prejudices at scale",
      "on_screen_text": "Bias in AI = AI inherits biases from training data",
      "analogies": ["child learning prejudice", "mirror reflecting society"],
      "related_terms": ["training_data", "fairness"],
      "visual_style": "problem_demonstration",
      "ascii_scenes": ["biased_data", "ai_learns", "biased_output"]
    },
    {
      "id": "llm",
      "name": "Large Language Model",
      "complexity": "medium",
      "duration": 25,
      "definition": "AI trained to predict and generate text",
      "hook": "ChatGPT, Claude, GPT-4 - they're all LLMs. Large Language Models. What does that mean?",
      "explanation": "An LLM is AI trained on massive amounts of text to predict what words come next. That's it. The magic comes from billions of parameters and training data.",
      "example": "You write 'The cat sat on the...' - LLM predicts 'mat' is likely. Scale this up billions of times, and you get human-like text generation.",
      "why_matters": "Understanding LLMs means understanding their strengths and fundamental limitations",
      "on_screen_text": "LLM = AI trained to predict and generate text",
      "analogies": ["autocomplete on steroids", "statistical parrot"],
      "related_terms": ["tokens", "hallucination", "temperature"],
      "visual_style": "demonstration",
      "ascii_scenes": ["text_input", "prediction_engine", "generated_text"]
    },
    {
      "id": "temperature",
      "name": "Temperature (AI)",
      "complexity": "medium",
      "duration": 25,
      "definition": "Controls AI randomness and creativity",
      "hook": "Why is AI sometimes boring and predictable, sometimes wild and creative? Temperature.",
      "explanation": "Temperature is a setting from 0 to 2. Low temperature (0.2) = predictable, safe outputs. High temperature (1.5) = creative, risky, sometimes nonsensical.",
      "example": "Temperature 0: AI always picks most likely next word. Boring but reliable. Temperature 2: AI picks random options. Creative but chaotic.",
      "why_matters": "Adjusting temperature lets you control AI creativity vs consistency",
      "on_screen_text": "Temperature = AI randomness control (0=boring, 2=wild)",
      "analogies": ["dial from safe to risky", "conservative vs experimental"],
      "related_terms": ["llm", "hallucination"],
      "visual_style": "comparison",
      "ascii_scenes": ["temp_zero", "vs", "temp_high"]
    },
    {
      "id": "prompt_engineering",
      "name": "Prompt Engineering",
      "complexity": "simple",
      "duration": 20,
      "definition": "Crafting inputs to get better AI outputs",
      "hook": "Same question, different wording, completely different AI answer. That's prompt engineering.",
      "explanation": "How you ask matters. Clear prompts get better results. Vague prompts get vague answers.",
      "example": "'Write code' vs 'Write Python code to sort a list of names alphabetically, with comments explaining each step.' Second one works better.",
      "why_matters": "Better prompts = better results. It's the new essential skill",
      "on_screen_text": "Prompt Engineering = Crafting inputs for better AI outputs",
      "analogies": ["asking good questions", "giving clear instructions"],
      "related_terms": ["llm", "few_shot_learning"],
      "visual_style": "comparison",
      "ascii_scenes": ["bad_prompt", "vs", "good_prompt"]
    },
    {
      "id": "embeddings",
      "name": "Embeddings",
      "complexity": "complex",
      "duration": 30,
      "definition": "Converting text/images into number arrays AI can compare",
      "hook": "How does AI know 'king' is to 'queen' as 'man' is to 'woman'? Embeddings.",
      "explanation": "Embeddings convert words, sentences, or images into lists of numbers (vectors). Similar meanings = similar numbers. AI can then mathematically compare meaning.",
      "example": "Word 'cat' becomes [0.2, -0.5, 0.8, ...]. Word 'dog' becomes [0.3, -0.4, 0.7, ...]. Close numbers = similar meaning.",
      "why_matters": "Embeddings enable search, recommendations, and AI understanding of similarity",
      "on_screen_text": "Embeddings = Converting text/images to comparable numbers",
      "analogies": ["GPS coordinates for meaning", "translating to math"],
      "related_terms": ["vector_database", "rag"],
      "visual_style": "process",
      "ascii_scenes": ["text_in", "number_conversion", "vector_out"]
    },
    {
      "id": "rag",
      "name": "RAG (Retrieval Augmented Generation)",
      "complexity": "complex",
      "duration": 30,
      "definition": "Giving AI access to external information before generating answers",
      "hook": "How do you give AI knowledge it wasn't trained on? RAG - Retrieval Augmented Generation.",
      "explanation": "RAG first searches a database for relevant info, then gives that info to the AI to generate an answer. It's like giving AI a textbook before asking questions.",
      "example": "Ask about your company docs. RAG searches docs, finds relevant sections, feeds them to AI, AI answers using those docs. Without RAG, AI would hallucinate.",
      "why_matters": "RAG lets AI work with current, private, or specialized knowledge",
      "on_screen_text": "RAG = Searching first, then generating with that info",
      "analogies": ["open-book exam", "looking it up before answering"],
      "related_terms": ["embeddings", "vector_database", "hallucination"],
      "visual_style": "process",
      "ascii_scenes": ["query", "retrieve_docs", "generate_with_context"]
    },
    {
      "id": "fine_tuning",
      "name": "Fine-tuning",
      "complexity": "medium",
      "duration": 25,
      "definition": "Specializing a pre-trained AI model for specific tasks",
      "hook": "Take a general AI, teach it your specific use case. That's fine-tuning.",
      "explanation": "Fine-tuning starts with a model already trained on general data, then trains it further on specialized data. Like teaching a doctor to be a surgeon.",
      "example": "GPT-4 is general purpose. Fine-tune it on medical journals, it becomes better at medical questions. Less data needed than training from scratch.",
      "why_matters": "Fine-tuning is cheaper and faster than training new models",
      "on_screen_text": "Fine-tuning = Specializing pre-trained AI for specific tasks",
      "analogies": ["medical school after college", "specialization"],
      "related_terms": ["training_data", "transfer_learning"],
      "visual_style": "process",
      "ascii_scenes": ["base_model", "specialized_training", "specialized_model"]
    },
    {
      "id": "transformer",
      "name": "Transformer",
      "complexity": "complex",
      "duration": 30,
      "definition": "Neural network architecture that revolutionized AI",
      "hook": "ChatGPT, DALL-E, AlphaFold - all powered by one architecture: Transformers.",
      "explanation": "Transformers are a neural network design from 2017 that processes all input at once (not word by word). They use 'attention' to focus on relevant parts.",
      "example": "The 'T' in GPT stands for Transformer. Before transformers, AI processed text sequentially. Transformers process in parallel - much faster, much better.",
      "why_matters": "Transformers enabled the current AI revolution. GPT, BERT, everything modern uses them",
      "on_screen_text": "Transformer = Neural network architecture behind modern AI",
      "analogies": ["reading whole paragraph vs word-by-word"],
      "related_terms": ["attention_mechanism", "llm"],
      "visual_style": "demonstration",
      "ascii_scenes": ["input_all_at_once", "attention", "output"]
    }
  ]
}
