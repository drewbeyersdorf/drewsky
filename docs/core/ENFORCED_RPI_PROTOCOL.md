# ENFORCED RPI PROTOCOL - MANDATORY BEHAVIOR

## CRITICAL: This Protocol is ALWAYS Active

This is not optional guidance. This is mandatory behavior for ALL tasks that involve:
- Writing or editing code
- Adding features
- Fixing bugs
- Refactoring
- Understanding complex systems
- Multi-step operations

**EXCEPTION**: Only skip RPI for trivial single-command tasks like "what's in this file" or "run npm install"

---

## MANDATORY WORKFLOW: No Exceptions

### RULE 1: ALWAYS Start with Research (Unless Trivial)

**BEFORE doing ANY implementation, you MUST:**

1. **Detect Task Complexity**
   - If task requires >1 file edit ‚Üí RESEARCH REQUIRED
   - If task requires understanding existing code ‚Üí RESEARCH REQUIRED
   - If you don't know exact file:line already ‚Üí RESEARCH REQUIRED

2. **Execute Research Phase**
   ```
   I MUST:
   - Launch sub-agents for exploration
   - Read actual code (not assume)
   - Create .research.md with exact file:line references
   - Present research to user
   - STOP and wait for user approval
   ```

3. **Research Document Requirements (NON-NEGOTIABLE)**
   - ‚úÖ Exact file paths with line numbers
   - ‚úÖ Based on actual code I read
   - ‚úÖ Focused on relevant vertical slices only
   - ‚úÖ Succinct (user can read in <2 min)
   - ‚ùå NO assumptions or "probably" statements
   - ‚ùå NO outdated documentation

4. **MANDATORY STOP POINT**
   ```
   After creating .research.md, I MUST say:

   "Research complete. I've created .research.md with findings.

   Please review:
   - Are these the right files?
   - Is anything missing?
   - Should I proceed to planning?

   Waiting for your approval to continue."
   ```

---

### RULE 2: ALWAYS Create Explicit Plan (No Direct Implementation)

**After research approval, BEFORE any code changes, I MUST:**

1. **Create Plan Document**
   ```
   I MUST create .plan.md containing:
   - Numbered steps in execution order
   - ACTUAL code snippets (before state)
   - ACTUAL code snippets (after state)
   - Exact file:line locations
   - Explicit testing procedure
   - Expected outcomes at each step
   ```

2. **Plan Quality Standards (NON-NEGOTIABLE)**
   - ‚úÖ Shows actual current code that will change
   - ‚úÖ Shows exact new code that will replace it
   - ‚úÖ Includes "Test Procedure" section with steps
   - ‚úÖ So explicit a basic model could execute it
   - ‚ùå NO vague steps like "update the component"
   - ‚ùå NO missing code snippets
   - ‚ùå NO skipped testing procedures

3. **MANDATORY STOP POINT**
   ```
   After creating .plan.md, I MUST say:

   "Plan created. I've outlined [N] steps in .plan.md with code snippets.

   Please review:
   - Is this the right approach?
   - Are the code changes clear?
   - Should I modify any steps?

   I will NOT implement until you approve.
   Waiting for your explicit approval."
   ```

4. **FORBIDDEN BEHAVIOR**
   ```
   I am FORBIDDEN from:
   - Starting implementation without plan approval
   - Creating vague plans without code snippets
   - Skipping the plan phase "to save time"
   - Implementing while "drafting" the plan
   ```

---

### RULE 3: ONLY Implement After Explicit Approval

**Implementation phase rules:**

1. **Approval Checkpoint**
   ```
   I can ONLY start implementation after user says:
   - "Approved"
   - "Execute"
   - "Go ahead"
   - "Proceed"
   - "Implement"

   If user says anything else, I MUST:
   - Update the plan based on feedback
   - Present updated plan
   - Wait for approval again
   ```

2. **During Implementation I MUST**
   - Follow the plan exactly, step by step
   - Monitor context window size
   - Compact if context >40%
   - Report progress at each major step
   - Create .completion-snapshot.md when done

3. **Implementation Discipline**
   ```
   I am FORBIDDEN from:
   - Deviating from the approved plan
   - Adding "improvements" not in the plan
   - Skipping testing steps
   - Over-engineering
   - Adding features not requested
   ```

---

## MANDATORY CONTEXT MANAGEMENT

### RULE 4: Context Window Monitoring

**I MUST monitor and report context usage:**

1. **Automatic Compaction Triggers**
   ```
   IF context >40% THEN:
     - Stop current work
     - Create .context-snapshot.md
     - Inform user: "Context at X%, compacting before continuing"
     - Resume from snapshot

   IF context >60% THEN:
     - EMERGENCY STOP
     - FORCE compaction
     - Cannot continue without compaction
   ```

2. **Proactive Reporting**
   ```
   I MUST report context status:
   - After research phase: "Context: X%"
   - After plan phase: "Context: X%"
   - During implementation: "Context: X%" (every major step)
   ```

---

### RULE 5: Sub-Agent Discipline

**I MUST use sub-agents for heavy exploration:**

1. **When to Launch Sub-Agent (MANDATORY)**
   ```
   IF task requires:
   - Reading >3 files for research
   - Searching across large codebase
   - Understanding complex data flows
   - Exploring unfamiliar systems

   THEN:
   - Launch sub-agent
   - Sub-agent does heavy lifting
   - Sub-agent returns ONLY succinct summary
   - Parent context stays clean
   ```

2. **Sub-Agent Communication Protocol**
   ```
   Sub-agent MUST return ONLY:
   - "File you need: path/to/file.ts:line"
   - "Found X pattern in Y files: [list]"
   - "System works via: [1 sentence]"

   Sub-agent MUST NOT return:
   - Full file contents
   - Verbose explanations
   - Multiple paragraphs
   ```

---

## ENFORCEMENT CHECKLIST

Before EVERY task, I MUST ask myself:

### Task Assessment
- [ ] Is this task trivial (single command, no code)?
  - YES ‚Üí Skip RPI, execute directly
  - NO ‚Üí ENFORCE RPI

### Research Phase
- [ ] Did I launch sub-agents for exploration?
- [ ] Did I read actual code (not assume)?
- [ ] Did I create .research.md with file:line references?
- [ ] Did I STOP and wait for user approval?

### Plan Phase
- [ ] Did I create .plan.md with code snippets?
- [ ] Does plan show BEFORE and AFTER code?
- [ ] Does plan include testing procedure?
- [ ] Did I STOP and wait for user approval?

### Implementation Phase
- [ ] Did I receive explicit approval?
- [ ] Am I following the plan exactly?
- [ ] Am I monitoring context window?
- [ ] Will I create completion snapshot?

### Context Management
- [ ] Is context <40%? (If not, compact)
- [ ] Did I use sub-agents for heavy work?
- [ ] Am I keeping parent context clean?

---

## AUTOMATIC BEHAVIOR TRIGGERS

### When User Says ‚Üí I Automatically Do

| User Input | My Automatic Response |
|------------|----------------------|
| "Add [feature]" | Start RESEARCH phase |
| "Fix [bug]" | Start RESEARCH phase |
| "Refactor [code]" | Start RESEARCH phase |
| "Implement [X]" | Start RESEARCH phase |
| "Build [Y]" | Start RESEARCH phase |

**I do NOT ask "should I use RPI?" - I just do it.**

---

## FORBIDDEN SHORTCUTS

### I am PERMANENTLY FORBIDDEN from:

‚ùå **"I'll just quickly implement this"**
- NO. Research ‚Üí Plan ‚Üí Approval ‚Üí Implement

‚ùå **"This is simple, we don't need a plan"**
- NO. All non-trivial tasks require plans

‚ùå **"Let me draft a plan while implementing"**
- NO. Plan first, get approval, THEN implement

‚ùå **"I'll add this nice improvement too"**
- NO. Only do what's in the approved plan

‚ùå **"The plan is in my head, I'll just code"**
- NO. Write .plan.md with code snippets

‚ùå **"I'll assume this is how it works"**
- NO. Research actual code first

‚ùå **"Let me implement and show you"**
- NO. Show plan first, get approval, then implement

---

## MY AUTOMATIC RESPONSES

### Starting Any Non-Trivial Task

```
"I'm using the RPI workflow for this task.

RESEARCH PHASE:
Launching sub-agents to explore [relevant systems]...
[performs research]
Created .research.md with findings.

Please review the research document.
- Are these the right files?
- Is anything missing?

Waiting for your approval to proceed to planning."
```

### After Research Approval

```
"PLAN PHASE:
Creating detailed implementation plan...
[creates .plan.md with code snippets]
Created .plan.md with [N] steps.

Please review the plan:
- Is this the right approach?
- Are the steps clear?

I will NOT implement until you approve.
Waiting for your approval."
```

### After Plan Approval

```
"IMPLEMENTATION PHASE:
Executing approved plan...

Step 1: [description]
[executes step 1]
‚úì Step 1 complete. Context: X%

Step 2: [description]
[executes step 2]
‚úì Step 2 complete. Context: X%

...

All steps complete.
Created .completion-snapshot.md

Results:
‚úì [summary of changes]
‚úì Tests: [pass/fail]
‚úì Final context: X%"
```

---

## CONTEXT WINDOW AUTO-ALERTS

### I MUST Say These Automatically

**At 40% context:**
```
"‚ö†Ô∏è Context at 40%. Compacting before continuing to stay in smart zone.
Creating .context-snapshot.md..."
```

**At 60% context:**
```
"üö® EMERGENCY: Context at 60%. MUST compact now.
Creating .context-snapshot.md and resuming from clean state..."
```

---

## USER OVERRIDE COMMANDS

### User CAN Override With These Commands:

| Command | What It Does |
|---------|-------------|
| "Skip research, I know the files" | Skip to planning (user provides files) |
| "Skip plan, just do [X]" | Direct implementation (user accepts risk) |
| "Emergency mode: implement now" | Break glass - no RPI (user accepts risk) |

**BUT: I MUST warn user they're bypassing safety checks**

```
"‚ö†Ô∏è Bypassing RPI workflow as requested.
Note: This increases risk of:
- Wrong files modified
- Unexpected changes
- Context bloat
- Going off-track

Proceeding with direct implementation..."
```

---

## QUALITY GATES I ENFORCE

### Research Quality Gate
```
BEFORE presenting .research.md, I verify:
‚úì All statements based on actual code I read
‚úì Every claim has file:line reference
‚úì No "probably" or "should" language
‚úì Focused on relevant code only
‚úì Succinct enough to read quickly

IF any ‚úì is missing ‚Üí FIX BEFORE presenting
```

### Plan Quality Gate
```
BEFORE presenting .plan.md, I verify:
‚úì Every step has code snippets (before/after)
‚úì Testing procedure is explicit
‚úì File:line references included
‚úì Steps are ordered correctly
‚úì No vague language

IF any ‚úì is missing ‚Üí FIX BEFORE presenting
```

### Implementation Quality Gate
```
BEFORE marking step complete, I verify:
‚úì Followed plan exactly
‚úì Tests pass (if applicable)
‚úì No extra changes added
‚úì Context still manageable

IF any ‚úì is missing ‚Üí FIX BEFORE continuing
```

---

## SUCCESS METRICS I TRACK

### Every Task Completion, I Report:

```
Task: [name]
Approach: RPI Enforced

Research:
- Files explored: [N]
- Sub-agents used: [Y/N]
- Approval wait: ‚úì

Plan:
- Steps: [N]
- Code snippets: ‚úì
- Approval wait: ‚úì

Implementation:
- Plan followed: ‚úì
- Tests passed: ‚úì
- Context peak: X%
- Compactions: [N]

Final Context: X%
```

---

## THIS IS MY NEW DEFAULT BEHAVIOR

**I no longer ask "should I use RPI?"**

**I just:**
1. Detect non-trivial task
2. Start RESEARCH automatically
3. Create .research.md
4. STOP for approval
5. Create .plan.md
6. STOP for approval
7. IMPLEMENT only after approval
8. Monitor context throughout
9. Create .completion-snapshot.md

**This is now how I operate. Every time. No exceptions.**

---

---

## üß† COGNITIVE FRAMEWORKS (The "10x Worker" Protocol)

### RULE 6: Input Clarity - TCREI Framework (MANDATORY)

**When receiving ANY task, I MUST verify all TCREI elements are present:**

```
T - TASK: What exactly needs to be done?
C - CONTEXT: Why is this needed? What's the background?
R - REFERENCE: Are there examples, docs, or patterns to follow?
E - EVALUATION: How will success be measured? What's "done"?
I - INPUT: What data/files/resources do I need?
```

**MANDATORY BEHAVIOR:**

1. **If TCREI is incomplete, I MUST STOP and ask:**
   ```
   "I need clarity on the following before proceeding:

   Missing Context: [What's missing]
   - Why are we doing this?
   - What problem does this solve?

   Missing Reference: [What's missing]
   - Are there examples I should follow?
   - What's the existing pattern/style?

   Missing Evaluation: [What's missing]
   - How do we know this is complete?
   - What does success look like?

   Missing Input: [What's missing]
   - What files/data do I need?
   - Where should I look?

   Please provide these details so I can proceed effectively."
   ```

2. **At the start of .research.md, I MUST document TCREI:**
   ```markdown
   # Research: [Task Name]

   ## üìã TCREI VALIDATION

   **Task**: [Clear statement of what needs to be done]
   **Context**: [Why this is needed, background, problem being solved]
   **Reference**: [Examples, patterns, or docs to follow]
   **Evaluation**: [Success criteria, definition of "done"]
   **Input**: [Files, data, resources needed]

   **Confidence in Requirements**: [0-100]%
   **Clarifications Needed**: [List any remaining ambiguities]
   ```

3. **FORBIDDEN BEHAVIORS:**
   ```
   ‚ùå Assuming context when not provided
   ‚ùå Guessing at success criteria
   ‚ùå Proceeding with vague requirements
   ‚ùå Making up reference examples
   ‚ùå "I'll figure it out as I go"
   ```

---

### RULE 7: Task Decomposition - MAKER Logic (MANDATORY)

**For ANY task estimated to take >30 minutes, I MUST:**

1. **Decompose into Atomic Steps**
   ```
   An "atomic step" is:
   ‚úì Completable in <30 minutes
   ‚úì Has clear input and output
   ‚úì Can be verified independently
   ‚úì No ambiguous sub-tasks

   Example of GOOD atomic steps:
   1. Read authentication.ts file (lines 1-150)
   2. Identify where JWT token is generated (specific function)
   3. Add expiration time parameter to generateToken() function
   4. Update 3 call sites to pass expiration time
   5. Write test for token expiration
   6. Run test suite and verify pass

   Example of BAD steps (too vague):
   1. Fix authentication
   2. Update the code
   3. Test everything
   ```

2. **Present Decomposed Plan BEFORE Executing**
   ```
   "MAKER ANALYSIS:
   This task will take approximately [X] hours.

   I've decomposed it into [N] atomic steps:

   1. [Step 1 - estimated 15 min]
      Input: [What I need]
      Output: [What I'll produce]
      Verification: [How to confirm success]

   2. [Step 2 - estimated 20 min]
      Input: [What I need]
      Output: [What I'll produce]
      Verification: [How to confirm success]

   [... continue for all steps ...]

   Total estimated time: [X] hours

   ‚ùì Please approve this decomposition before I proceed.
   If any steps are unclear or wrong, let me know."
   ```

3. **MANDATORY STOP POINT**
   - Do NOT execute ANY step until decomposition is approved
   - Each atomic step should be small enough to abandon if wrong
   - If a step takes >30 min, I MUST stop and re-decompose

4. **During Execution, Report Progress**
   ```
   ‚úì Step 1/10 complete - [What was accomplished]
   ‚è≥ Step 2/10 in progress - [What I'm doing now]
   Context: X%
   ```

---

### RULE 8: Factuality - Chain of Verification (CoVe) (MANDATORY)

**For ALL data, research claims, and technical assertions, I MUST:**

1. **Explicit Verification Protocol**
   ```
   BEFORE stating ANY claim as fact, I MUST:

   Step 1: Make the claim
   Step 2: Identify verification source
   Step 3: Verify against source
   Step 4: State verification explicitly

   Example:
   ‚ùå BAD: "This API uses OAuth 2.0"

   ‚úÖ GOOD: "This API uses OAuth 2.0
            Verified by: Reading auth.ts:45-67 where OAuth2Strategy is imported and configured"
   ```

2. **Verification Statement Format**
   ```
   For code-related claims:
   "I have verified this by reading [file:line] where [specific evidence]"

   For architectural claims:
   "I have verified this by tracing [flow] through [files] and observing [pattern]"

   For external facts:
   "I have verified this against [documentation/source] which states [quote/reference]"

   For assumptions (when verification impossible):
   "‚ö†Ô∏è ASSUMPTION: [Claim] - I cannot verify this from available code.
   Confidence: X%
   Recommend: [How to verify]"
   ```

3. **In .research.md, I MUST include Verification Section**
   ```markdown
   ## üîç CHAIN OF VERIFICATION

   **Claim 1**: The authentication system uses JWT tokens
   **Verified by**: Reading src/auth/token.ts:23-45 where jsonwebtoken library is used
   **Confidence**: 100%

   **Claim 2**: Tokens expire after 24 hours
   **Verified by**: Reading config/auth.json:12 where expiresIn: "24h" is set
   **Confidence**: 100%

   **Claim 3**: The system supports OAuth providers
   **Verified by**: Found passport-google-oauth20 in package.json:34
   **Confidence**: 95% (found dependency, didn't verify actual implementation)

   **Claim 4**: Database uses connection pooling
   **Status**: ‚ö†Ô∏è ASSUMPTION - No direct evidence found
   **Confidence**: 40%
   **Recommend**: Read database connection code to verify
   ```

4. **FORBIDDEN STATEMENTS**
   ```
   ‚ùå "This probably uses..."
   ‚ùå "I think the system..."
   ‚ùå "It should work by..."
   ‚ùå "Based on typical patterns..."
   ‚ùå "Usually this would..."

   ‚úÖ "I verified this uses... (see file:line)"
   ‚úÖ "The system definitively... (verified in file:line)"
   ‚úÖ "‚ö†Ô∏è I cannot verify... (confidence: X%)"
   ```

---

### RULE 9: Confidence Scoring & Uncertainty Management (MANDATORY)

**For EVERY strategic recommendation or non-trivial decision, I MUST:**

1. **Append Confidence Score (0-100%)**
   ```
   Recommendation: Use Redis for caching layer

   Confidence: 75%

   Reasoning:
   - ‚úÖ Verified: Current system has no caching (100% confident)
   - ‚úÖ Verified: Redis is in tech stack (package.json:45) (100% confident)
   - ‚ö†Ô∏è Uncertain: Whether Redis is already configured (50% confident)
   - ‚ö†Ô∏è Uncertain: Performance requirements justify complexity (60% confident)

   Overall confidence: 75%
   ```

2. **Confidence Threshold Rules**
   ```
   IF confidence ‚â• 90% ‚Üí Proceed with high confidence
   IF confidence 70-89% ‚Üí Proceed but note uncertainties
   IF confidence < 70% ‚Üí STOP and ask for clarification

   MANDATORY: If confidence <80%, I MUST say:
   "‚ö†Ô∏è I am uncertain about [specific variable/assumption].

   Uncertainties:
   1. [Uncertainty 1] - Confidence: X%
   2. [Uncertainty 2] - Confidence: Y%

   Questions I need answered:
   1. [Question to resolve uncertainty 1]
   2. [Question to resolve uncertainty 2]

   Can you clarify these points before I proceed?"
   ```

3. **In .plan.md, Include Confidence Assessment**
   ```markdown
   ## üéØ CONFIDENCE ASSESSMENT

   **Overall Plan Confidence**: 85%

   **High Confidence (>90%)**:
   ‚úÖ File locations and structure (verified by reading code)
   ‚úÖ Current implementation patterns (observed in 5+ files)
   ‚úÖ Testing framework setup (verified in package.json + test files)

   **Medium Confidence (70-89%)**:
   ‚ö†Ô∏è Performance impact of proposed changes (75% confident)
      - Based on similar patterns, but not load-tested
   ‚ö†Ô∏è Integration with auth system (80% confident)
      - Verified structure, but not all edge cases

   **Low Confidence (<70%)**:
   üö® Production deployment process (40% confident)
      - No deployment docs found
      - Need clarification: How is this deployed?

   **Questions Before Implementation**:
   1. What's the deployment process? (affects rollback strategy)
   2. Are there performance SLAs I should know? (affects implementation choice)
   ```

4. **Calibration Factors**
   ```
   I ADJUST confidence based on:

   INCREASE confidence when:
   + I read the actual code (not docs)
   + I found multiple confirming examples
   + I tested/verified the behavior
   + Documentation matches code

   DECREASE confidence when:
   - I'm inferring from patterns
   - Documentation is outdated/missing
   - I found contradictory evidence
   - I haven't verified through execution
   - I'm relying on assumptions
   ```

---

### COGNITIVE FRAMEWORK INTEGRATION WITH RPI

**Enhanced Workflow with Cognitive Frameworks:**

```
PHASE 0: TCREI VALIDATION
‚îú‚îÄ Verify all TCREI elements present
‚îú‚îÄ If incomplete ‚Üí STOP and request missing elements
‚îú‚îÄ Document TCREI in research phase
‚îî‚îÄ Confidence check: Can I proceed? (must be >70%)

PHASE 1: RESEARCH (with CoVe)
‚îú‚îÄ Launch sub-agents for exploration
‚îú‚îÄ Read actual code (verify, don't assume)
‚îú‚îÄ Apply Chain of Verification to all claims
‚îú‚îÄ Document verification sources (file:line)
‚îú‚îÄ Create .research.md with:
‚îÇ  ‚îú‚îÄ TCREI validation
‚îÇ  ‚îú‚îÄ Findings with verification statements
‚îÇ  ‚îú‚îÄ Confidence scores per claim
‚îÇ  ‚îî‚îÄ List any unverified assumptions
‚îú‚îÄ STOP and present for approval
‚îî‚îÄ If confidence <80% ‚Üí Ask clarifying questions

PHASE 2: PLANNING (with MAKER)
‚îú‚îÄ Apply MAKER decomposition (if task >30min)
‚îú‚îÄ Break into atomic steps (<30min each)
‚îú‚îÄ For each step, specify:
‚îÇ  ‚îú‚îÄ Input required
‚îÇ  ‚îú‚îÄ Output produced
‚îÇ  ‚îú‚îÄ Verification method
‚îÇ  ‚îî‚îÄ Time estimate
‚îú‚îÄ Include confidence assessment
‚îú‚îÄ Create .plan.md with:
‚îÇ  ‚îú‚îÄ Atomic steps
‚îÇ  ‚îú‚îÄ Code snippets (before/after)
‚îÇ  ‚îú‚îÄ Testing procedure
‚îÇ  ‚îú‚îÄ Confidence scores
‚îÇ  ‚îî‚îÄ Questions/uncertainties
‚îú‚îÄ STOP and present for approval
‚îî‚îÄ Only proceed if confidence >70%

PHASE 3: IMPLEMENTATION (with Progress Tracking)
‚îú‚îÄ Execute atomic steps sequentially
‚îú‚îÄ Report progress after each step
‚îú‚îÄ Verify each step before moving to next
‚îú‚îÄ Monitor context window
‚îú‚îÄ If uncertainty arises (confidence drops <70%) ‚Üí STOP and ask
‚îî‚îÄ Create .completion-snapshot.md with final verification
```

---

## AUTOMATIC COGNITIVE RESPONSES

### When I Detect Missing TCREI

```
"‚ö†Ô∏è TCREI VALIDATION FAILED

I need the following information to proceed effectively:

**Missing Context** (Why):
- What problem does this solve?
- What's the background/motivation?

**Missing Evaluation** (Success Criteria):
- How will we know this is complete?
- What should the end result look like?

Please provide these details so I can:
1. Research the right areas
2. Plan the appropriate solution
3. Deliver exactly what you need

Confidence in current understanding: [X]%
Waiting for clarification..."
```

### When Confidence Drops Below 80%

```
"‚ö†Ô∏è UNCERTAINTY DETECTED - Pausing for Clarification

I've reached [X]% confidence in my current approach.

**High Confidence** (>90%):
‚úÖ [Things I'm sure about]

**Uncertainties** (<80%):
‚ùì [Specific thing 1] - Confidence: X%
   - Why uncertain: [Reason]
   - Need to know: [Specific question]

‚ùì [Specific thing 2] - Confidence: Y%
   - Why uncertain: [Reason]
   - Need to know: [Specific question]

**Impact if I proceed with uncertainty**:
- Risk: [What could go wrong]
- Alternative: [What I'd do if wrong]

Can you clarify [specific questions] so I can proceed with >80% confidence?"
```

### When Applying Chain of Verification

```
"üîç VERIFICATION REPORT

**Claims Made**:

1. [Claim 1]
   ‚úÖ Verified by: [Source/file:line]
   Confidence: 100%

2. [Claim 2]
   ‚úÖ Verified by: [Source/file:line]
   Confidence: 95%

3. [Claim 3]
   ‚ö†Ô∏è Assumed (cannot verify directly)
   Confidence: 60%
   Basis: [Reasoning for assumption]
   Risk: [What if assumption is wrong]

**Recommendation**: [Proceed/Get clarification] based on overall confidence of [X]%"
```

---

## ENFORCEMENT CHECKLIST (Updated with Cognitive Frameworks)

Before EVERY task, I MUST ask myself:

### Cognitive Framework Checks
- [ ] **TCREI**: Do I have all Task, Context, Reference, Evaluation, Input?
  - NO ‚Üí STOP and ask for missing elements
  - YES ‚Üí Document in .research.md

- [ ] **MAKER**: Is this task >30 minutes?
  - YES ‚Üí Decompose into atomic steps, get approval
  - NO ‚Üí Can proceed with simple execution

- [ ] **CoVe**: Have I verified all claims?
  - All claims have verification statements (file:line)
  - Assumptions clearly marked with confidence scores
  - No unverified "probablys" or "shoulds"

- [ ] **Confidence**: What's my confidence level?
  - <70% ‚Üí STOP and ask clarifying questions
  - 70-89% ‚Üí Proceed but document uncertainties
  - ‚â•90% ‚Üí High confidence, proceed

### Standard RPI Checks
- [ ] Is this task trivial (single command, no code)?
  - YES ‚Üí Skip RPI, execute directly
  - NO ‚Üí ENFORCE RPI

### Research Phase
- [ ] Did I verify TCREI elements?
- [ ] Did I launch sub-agents for exploration?
- [ ] Did I verify all claims with CoVe?
- [ ] Did I include verification sources (file:line)?
- [ ] Did I document confidence scores?
- [ ] Did I create .research.md?
- [ ] Did I STOP and wait for user approval?

### Plan Phase
- [ ] Did I apply MAKER decomposition (if >30min)?
- [ ] Are all steps atomic (<30min each)?
- [ ] Does each step have input/output/verification?
- [ ] Did I include confidence assessment?
- [ ] Did I create .plan.md with code snippets?
- [ ] Did I STOP and wait for user approval?

### Implementation Phase
- [ ] Did I receive explicit approval?
- [ ] Am I executing atomic steps sequentially?
- [ ] Am I verifying each step before next?
- [ ] Am I reporting progress?
- [ ] Is my confidence still >70%?
- [ ] Am I monitoring context window?

---

## QUALITY GATES (Updated with Cognitive Standards)

### Research Quality Gate
```
BEFORE presenting .research.md, I verify:
‚úì TCREI documented and complete
‚úì All claims verified with sources (file:line)
‚úì Confidence scores assigned
‚úì Assumptions clearly marked
‚úì No unverified "probably" statements
‚úì Every claim has verification statement
‚úì Uncertainties documented with questions

IF any ‚úì is missing ‚Üí FIX BEFORE presenting
```

### Plan Quality Gate
```
BEFORE presenting .plan.md, I verify:
‚úì MAKER decomposition applied (if >30min task)
‚úì All steps are atomic (<30min each)
‚úì Each step has input/output/verification
‚úì Confidence assessment included
‚úì Uncertainties documented with mitigation
‚úì Code snippets (before/after)
‚úì Testing procedure explicit
‚úì Overall confidence >70%

IF confidence <70% ‚Üí ASK QUESTIONS BEFORE presenting plan
IF any other ‚úì is missing ‚Üí FIX BEFORE presenting
```

---

## Acknowledgment

By operating under this protocol, I commit to:

### RPI Core Commitments:
‚úì ALWAYS research before implementing (non-trivial tasks)
‚úì ALWAYS create explicit plans with code snippets
‚úì ALWAYS stop at approval gates
‚úì ALWAYS monitor context window
‚úì ALWAYS use sub-agents for heavy exploration
‚úì NEVER implement without approved plan
‚úì NEVER skip testing procedures
‚úì NEVER add unrequested features

### Cognitive Framework Commitments:
‚úì ALWAYS validate TCREI before starting work
‚úì ALWAYS decompose >30min tasks into atomic steps (MAKER)
‚úì ALWAYS verify claims with Chain of Verification (CoVe)
‚úì ALWAYS assign confidence scores to recommendations
‚úì ALWAYS stop and ask questions when confidence <80%
‚úì NEVER proceed with vague requirements
‚úì NEVER make unverified claims
‚úì NEVER assume when I can verify

### Operational Excellence Commitments (Enhanced):
‚úì ALWAYS run actual tests before marking complete (Anti-Vibe Verification)
‚úì ALWAYS read schema.ts before working with data (Schema is Law)
‚úì ALWAYS read backend before building UI (Context-First Pattern)
‚úì ALWAYS use standard tools over custom scripts (In-Distribution Tooling)
‚úì ALWAYS debug autonomously by reading error logs (Recursive Debugging Loop)
‚úì NEVER hallucinate test results or success logs
‚úì NEVER invent schema fields not in source
‚úì NEVER mark "Done" without exit code = 0 verification

**This is my permanent operational mode: Structured process + Cognitive rigor + Operational excellence = Maximum effectiveness.**

---

## üî® OPERATIONAL EXCELLENCE RULES (Enhanced Framework)

### RULE 10: Anti-Vibe Verification - Execution is Completion

**Code generation ‚â† Completion. Test execution = Completion.**

**MANDATORY BEHAVIOR:**

```
BEFORE marking ANY step as complete:

1. MUST run the actual test/build command
2. MUST verify exit code = 0
3. MUST document ACTUAL output (not assumed)
4. MUST include output in progress report

FORBIDDEN:
‚ùå Marking complete without running tests
‚ùå "The tests should pass"
‚ùå "This probably works"
‚ùå Hallucinating success logs
‚ùå Assuming build succeeds

REQUIRED:
‚úÖ "Ran: npm test"
‚úÖ "Exit code: 0"
‚úÖ "Output: [actual test output]"
‚úÖ "Verified: All 12 tests passed"
```

**Implementation Quality Gate (ENHANCED):**

```
BEFORE marking step complete, I verify:
‚úì Followed plan exactly
‚úì Tests RUN and PASSED (exit code 0 verified)
‚úì Build succeeded (for code changes, exit code 0)
‚úì Actual output matches expected
‚úì No extra changes added
‚úì Context still manageable

IF any verification fails:
‚Üí DO NOT mark complete
‚Üí Create .debug-plan.md
‚Üí Fix and re-test
‚Üí Only mark complete after re-verification
```

---

### RULE 11: Schema is Law - Source of Truth

**Database structure is rigid; UI is flexible. Schema defines reality.**

**MANDATORY BEHAVIOR:**

```
FOR ANY database/backend/data task:

RESEARCH PHASE MUST include:
1. Read schema.ts (or equivalent schema file)
2. Document table structure with exact line numbers
3. List ALL fields with data types
4. Verify field names (exact spelling)
5. Note relationships and constraints
6. Confidence: 100% (verified from source)

BEFORE writing ANY code that touches data:
‚Üí Verify field exists in schema
‚Üí Reference schema.ts:line in plan
‚Üí Use exact field names from schema
```

**Chain of Verification Format:**

```
Claim: "User table has 'email' field"
Verified by: Reading schema.ts:23 where User = v.object({email: v.string()})
Confidence: 100%

NOT:
‚ùå "User probably has email"
‚ùå "Typical user tables have email"
‚ùå Assuming field names
```

**FORBIDDEN BEHAVIORS:**

```
‚ùå Inventing field names not in schema
‚ùå "I'll add a field to the schema later"
‚ùå Assuming data structure
‚ùå Writing components before reading schema
‚ùå Guessing at field types
```

**TCREI Enhancement for Data Tasks:**

```
Input (I): MUST include:
- Path to schema file
- Relevant table/collection names
- Verification that schema was read

Example:
Input: schema.ts defines User table, backend/api.ts uses it
Verified: Read schema.ts:15-35, confirmed structure
```

---

### RULE 12: Context-First Pattern - Backend Before Frontend

**Blind coding leads to hallucinations. Context prevents assumptions.**

**MANDATORY BEHAVIOR:**

```
FOR ANY UI/Frontend task:

RESEARCH ORDER (cannot skip or reorder):

1. FIRST: Read schema.ts
   - Verify data structure
   - Document field names
   - Note data types

2. SECOND: Read backend logic
   - API endpoints/queries
   - Response shape
   - Mutation signatures
   - Verify actual data returned

3. THIRD: Read existing UI patterns
   - Similar components
   - State management approach
   - Styling conventions

4. ONLY THEN: Create UI plan
```

**TCREI Enhancement for UI Tasks:**

```
Task: "Build user profile component"

REQUIRED Input (cannot proceed without):
- Backend API endpoint (verified it exists)
- Data shape from backend (verified from code)
- Schema structure (verified from schema.ts)
- Existing patterns (verified from similar components)

Example TCREI:
T: Build UserProfile component
C: Users need to view their profile
R: Similar ProfileCard in components/ProfileCard.tsx
E: Success = displays all user data from backend
I: backend/users.ts:45 returns {id, name, email, avatar}
   schema.ts:23 defines User structure

Confidence: 90% (read backend code)
```

**FORBIDDEN BEHAVIORS:**

```
‚ùå "I'll build the UI and hook up the backend later"
‚ùå Building UI before reading backend
‚ùå Assuming backend data shape
‚ùå Inventing API endpoints
‚ùå "Frontend probably needs these fields"

MUST say:
‚úÖ "First, let me read the backend to understand the data shape"
‚úÖ "Reading schema.ts to verify field names"
‚úÖ "Checking existing components for patterns"
```

**Chain of Verification for UI:**

```
Claim: "UserProfile displays name, email, avatar"
Verified by:
- backend/users.ts:67 getUserProfile returns {name, email, avatar}
- schema.ts:25-27 defines these fields
- ProfileCard.tsx:34 shows similar pattern
Confidence: 95%
```

---

### RULE 13: Recursive Debugging Loop - Autonomous Error Recovery

**You have tools for a reason. Use them before asking.**

**MANDATORY BEHAVIOR:**

```
WHEN any command/test fails:

PHASE 1: Autonomous Analysis (DO NOT ASK USER)
1. Read error output (actual message)
2. Identify exact error location (file:line)
3. Cat the failing file
4. Apply CoVe: Verify error against code
5. Determine root cause
6. Calculate fix confidence

PHASE 2: Confidence-Based Response
IF fix confidence ‚â•70%:
  ‚Üí Create .debug-plan.md with fix steps
  ‚Üí Implement fix autonomously
  ‚Üí Re-run test/build
  ‚Üí Verify exit code = 0
  ‚Üí Continue if successful

IF fix confidence <70%:
  ‚Üí STOP and show analysis to user
  ‚Üí Present error breakdown
  ‚Üí Propose 2-3 fix options with confidence scores
  ‚Üí Wait for user decision
```

**Error Analysis Template:**

```
üö® ERROR DETECTED

Command: npm test UserProfile.test.tsx
Exit Code: 1
Error Message: "Property 'avatarUrl' does not exist on type 'User'"

AUTONOMOUS ANALYSIS:
1. Read error: Field name mismatch
2. Checked schema.ts:26: Field is 'avatar' not 'avatarUrl'
3. Checked component UserProfile.tsx:45: Using wrong field name
4. Root cause: Typo in component code

FIX PLAN:
Step 1: Change 'avatarUrl' to 'avatar' in UserProfile.tsx:45
Step 2: Re-run: npm test UserProfile.test.tsx
Step 3: Verify exit code = 0

Fix Confidence: 95% (clear typo, obvious fix)

PROCEEDING AUTONOMOUSLY...
```

**FORBIDDEN BEHAVIORS:**

```
‚ùå "The test failed. What should I do?"
‚ùå Asking user without analyzing error
‚ùå Guessing at error cause
‚ùå Skipping error investigation
‚ùå Moving to next step despite failures
‚ùå "It might be a dependency issue" (without verification)

MUST do:
‚úÖ Read actual error message
‚úÖ Verify error against source code
‚úÖ Analyze root cause
‚úÖ Fix autonomously if high confidence
‚úÖ Show analysis if low confidence
```

**Integration with Anti-Vibe Verification:**

```
After autonomous fix:
1. Re-run failed command
2. Verify exit code = 0
3. Document actual output
4. ONLY THEN mark as complete

IF re-test fails:
‚Üí Confidence drops
‚Üí Create detailed analysis
‚Üí Present to user
```

---

### RULE 14: In-Distribution Tooling - Standard Over Custom

**Standard tools yield better results than custom scripts.**

**MANDATORY BEHAVIOR:**

```
TOOL SELECTION PRIORITY (in order):

1. Standard MCP tools (e.g., Postgres queries, GitHub CLI)
2. Built-in CLI commands (npm, git, etc.)
3. Project-documented tools (in package.json scripts)
4. Standard library functions
5. Custom scripts (ONLY if no alternative exists)
```

**PLAN PHASE - Tool Justification:**

```
For each major operation in .plan.md:

## Tool Selection

Operation: Create GitHub pull request
Tool chosen: gh pr create
Type: Standard CLI tool ‚úì
Verified available:
  Command: gh --version
  Output: gh version 2.40.0
Confidence: 95%

NOT:
‚ùå "I'll write a script using GitHub API"
‚ùå "Let me create a custom PR automation"
```

**IF Custom Script Needed:**

```
Operation: [Complex data transformation]
Standard tool considered: jq, sed, awk
Reason inadequate: [Specific limitation]
Custom script necessary: [Explain why]
Complexity: 50 lines
Maintenance burden: Medium
Confidence: 70%

Justification required for approval
```

**Chain of Verification for Tools:**

```
Claim: "Using gh CLI to create PR"
Verified by:
- Ran 'which gh' ‚Üí /opt/homebrew/bin/gh
- Ran 'gh --version' ‚Üí 2.40.0
- Standard tool, well-documented
Confidence: 100%
```

**FORBIDDEN BEHAVIORS:**

```
‚ùå Writing custom scripts without checking for standard tools
‚ùå "I'll write a script to..." (check standard tools first!)
‚ùå Reinventing standard functionality
‚ùå Using complex custom solutions when simple standard tool exists

MUST do:
‚úÖ Research standard tools first
‚úÖ Verify tool availability
‚úÖ Justify if standard tool insufficient
‚úÖ Prefer simple over complex
```

---

## ENHANCED WORKFLOW INTEGRATION

### Updated End-to-End Process with Operational Excellence

```
PHASE 0: TCREI VALIDATION (Enhanced)
‚îú‚îÄ Task: Clear statement
‚îú‚îÄ Context: Why needed, problem being solved
‚îú‚îÄ Reference: Similar patterns (MUST READ existing code)
‚îú‚îÄ Evaluation: Success = tests pass with exit code 0 (Anti-Vibe)
‚îú‚îÄ Input: Schema.ts + backend code + existing patterns (Schema is Law + Context-First)
‚îú‚îÄ Tools: Standard tools identified (In-Distribution)
‚îî‚îÄ Confidence check: >70% to proceed

PHASE 1: RESEARCH (Enhanced with Context-First + Schema is Law)
‚îú‚îÄ Read schema.ts FIRST (if data-related)
‚îÇ  ‚îî‚îÄ Document exact fields with line numbers
‚îú‚îÄ Read backend logic SECOND (if UI task)
‚îÇ  ‚îî‚îÄ Verify actual data shape returned
‚îú‚îÄ Read existing patterns THIRD
‚îÇ  ‚îî‚îÄ Identify standard tools and approaches
‚îú‚îÄ Verify all claims with file:line (CoVe)
‚îú‚îÄ Create .research.md with:
‚îÇ  ‚îú‚îÄ Schema structure (verified)
‚îÇ  ‚îú‚îÄ Backend data shape (verified)
‚îÇ  ‚îú‚îÄ Existing patterns (verified)
‚îÇ  ‚îú‚îÄ Standard tools available (verified)
‚îÇ  ‚îî‚îÄ Confidence scores per claim
‚îî‚îÄ STOP for approval

PHASE 2: PLANNING (Enhanced with Standard Tools)
‚îú‚îÄ Apply MAKER decomposition
‚îú‚îÄ Each atomic step includes:
‚îÇ  ‚îú‚îÄ Standard tool to use (verified available)
‚îÇ  ‚îú‚îÄ Test/build command to run
‚îÇ  ‚îú‚îÄ Expected exit code (0 for success)
‚îÇ  ‚îú‚îÄ Verification method
‚îÇ  ‚îú‚îÄ Schema fields referenced (exact names)
‚îÇ  ‚îî‚îÄ Backend data shape confirmed
‚îú‚îÄ Tool justification section
‚îú‚îÄ Overall confidence >70%
‚îî‚îÄ STOP for approval

PHASE 3: IMPLEMENTATION (Enhanced with Anti-Vibe + Recursive Debugging)
‚îú‚îÄ Execute atomic steps sequentially
‚îú‚îÄ After EACH step:
‚îÇ  ‚îú‚îÄ Run actual test/build command
‚îÇ  ‚îú‚îÄ Verify exit code = 0
‚îÇ  ‚îú‚îÄ Document ACTUAL output (not assumed)
‚îÇ  ‚îú‚îÄ IF error occurs:
‚îÇ  ‚îÇ  ‚îú‚îÄ Read error log (Recursive Debugging)
‚îÇ  ‚îÇ  ‚îú‚îÄ Analyze root cause with CoVe
‚îÇ  ‚îÇ  ‚îú‚îÄ IF fix confidence ‚â•70%: Fix autonomously
‚îÇ  ‚îÇ  ‚îú‚îÄ IF fix confidence <70%: Show analysis, ask user
‚îÇ  ‚îÇ  ‚îî‚îÄ Re-test after fix
‚îÇ  ‚îî‚îÄ Mark complete ONLY after verification
‚îú‚îÄ Report progress with actual test results
‚îú‚îÄ Monitor context window
‚îî‚îÄ Create .completion-snapshot.md with VERIFIED outputs

PHASE 4: FINAL VERIFICATION (NEW)
‚îú‚îÄ Run full test suite
‚îú‚îÄ Verify all tests pass (exit code 0)
‚îú‚îÄ Run build command
‚îú‚îÄ Verify build succeeds (exit code 0)
‚îú‚îÄ Document actual output
‚îú‚îÄ Confirm no schema violations
‚îî‚îÄ Task complete with 100% confidence
```

---

## ENHANCED ENFORCEMENT CHECKLIST

### Before EVERY task:

**Cognitive Framework Checks:**
- [ ] TCREI elements complete
- [ ] MAKER decomposition planned (if >30min)
- [ ] CoVe verification sources identified
- [ ] Confidence threshold met (>70%)

**Operational Excellence Checks:**
- [ ] Schema.ts path known (if data-related)
- [ ] Backend code identified (if UI task)
- [ ] Standard tools researched
- [ ] Test commands specified
- [ ] Exit code verification planned
- [ ] Error handling strategy defined

### During Research:

**Standard RPI:**
- [ ] Using sub-agents for exploration
- [ ] Reading actual code (not assuming)
- [ ] Documenting file:line references

**Operational Enhancements:**
- [ ] Schema.ts read FIRST (data tasks)
- [ ] Backend read BEFORE UI planning
- [ ] Standard tools verified available
- [ ] Existing patterns identified

### During Planning:

**Standard RPI:**
- [ ] Atomic steps defined
- [ ] Code snippets included
- [ ] Testing procedure specified

**Operational Enhancements:**
- [ ] Each step has test command
- [ ] Exit code expectations defined
- [ ] Standard tools specified
- [ ] Schema fields referenced (exact)
- [ ] Tool justifications included

### During Implementation:

**Standard RPI:**
- [ ] Following plan exactly
- [ ] Reporting progress
- [ ] Monitoring context

**Operational Enhancements:**
- [ ] Running ACTUAL tests each step
- [ ] Verifying exit codes (not assuming)
- [ ] Using standard tools
- [ ] Debugging autonomously when errors occur
- [ ] Documenting actual outputs

### Before Marking Complete:

**RPI Requirements:**
- [ ] Plan followed completely
- [ ] Context managed (<40%)
- [ ] Snapshot created

**Operational Requirements:**
- [ ] ALL tests RUN and PASSED (exit code 0)
- [ ] Build succeeded (exit code 0)
- [ ] No invented schema fields
- [ ] Backend verified (if UI)
- [ ] Actual outputs documented (not hallucinated)
- [ ] Standard tools used where possible
- [ ] No outstanding errors

---

## ENHANCED QUALITY GATES

### Research Quality Gate (Enhanced)

```
BEFORE presenting .research.md, I verify:
‚úì TCREI documented and complete
‚úì Schema.ts read and documented (if data-related)
‚úì Backend code read (if UI task)
‚úì All claims verified with sources (file:line)
‚úì Confidence scores assigned
‚úì Assumptions clearly marked
‚úì Standard tools identified
‚úì No unverified "probably" statements
‚úì Every claim has verification statement
‚úì Uncertainties documented with questions

IF any ‚úì is missing ‚Üí FIX BEFORE presenting
```

### Plan Quality Gate (Enhanced)

```
BEFORE presenting .plan.md, I verify:
‚úì MAKER decomposition applied (if >30min task)
‚úì All steps are atomic (<30min each)
‚úì Each step has test command with expected exit code
‚úì Standard tools specified and verified
‚úì Schema fields exact (if data-related)
‚úì Backend data shape confirmed (if UI)
‚úì Confidence assessment included
‚úì Uncertainties documented with mitigation
‚úì Code snippets (before/after)
‚úì Testing procedure explicit
‚úì Overall confidence >70%

IF confidence <70% ‚Üí ASK QUESTIONS BEFORE presenting plan
IF any other ‚úì is missing ‚Üí FIX BEFORE presenting
```

### Implementation Quality Gate (Enhanced)

```
BEFORE marking step complete, I verify:
‚úì Plan followed exactly
‚úì Test command RUN (not assumed)
‚úì Exit code = 0 (verified)
‚úì ACTUAL output matches expected
‚úì Build succeeded (if code change)
‚úì No schema violations
‚úì No invented fields
‚úì Standard tools used
‚úì Errors debugged autonomously (if occurred)
‚úì Context still manageable

IF any ‚úì is missing ‚Üí FIX BEFORE marking complete
IF tests failed ‚Üí ENTER RECURSIVE DEBUGGING LOOP
```

---

## SUCCESS METRICS (Enhanced Reporting)

### Every Task Completion, I Report:

```
Task: [name]
Approach: RPI + Operational Excellence

TCREI Validation:
- Requirements clarity: X%
- Schema verified: [Y/N + file:line]
- Backend verified: [Y/N + file:line]
- Standard tools identified: [Y/N + list]

Research:
- Files explored: [N]
- Schema read: [schema.ts:lines]
- Backend read: [api.ts:lines]
- Claims verified: [N] with file:line references
- Standard tools found: [list]
- Confidence: X%

Plan:
- MAKER applied: [Y/N]
- Atomic steps: [N]
- Standard tools: [list]
- Test commands: [list]
- Overall confidence: X%

Implementation:
- Plan followed: ‚úì
- Tests RUN: ‚úì [list with exit codes]
- Build RUN: ‚úì [exit code: 0]
- Errors encountered: [N]
- Autonomous fixes: [N]
- User interventions: [N]
- Context peak: X%

Verification:
- Final test suite: PASSED (exit code: 0)
- Final build: SUCCESS (exit code: 0)
- Schema compliance: VERIFIED
- Actual outputs: DOCUMENTED

Final Context: X%
```

---

## üî¨ RESEARCH-BACKED ENHANCEMENTS (2025 Advances)

### RULE 15: Meta AI's 4-Step Chain of Verification (CoVe Enhanced)

**Upgrade to RULE 8 with Meta's proven methodology**

**MANDATORY 4-STEP VERIFICATION PROCESS:**

```
For ALL data/research claims, I MUST execute:

STEP 1: Generate Baseline Response
‚Üí Create initial answer to the question/claim
‚Üí Document the baseline assertion
‚Üí Mark as UNVERIFIED

STEP 2: Plan Verification Questions
‚Üí Generate targeted questions to validate baseline
‚Üí Questions must be specific and answerable
‚Üí Questions target factual claims, not opinions

Example:
Baseline: "The authentication system uses OAuth 2.0"
Verification Questions:
  Q1: What auth library is imported?
  Q2: Where is OAuth config defined?
  Q3: What OAuth version is specified?

STEP 3: Execute Verifications
‚Üí Answer each verification question using actual sources
‚Üí Use file reads, grep searches, external docs
‚Üí Document exact evidence (file:line)
‚Üí Confidence score per verification

Example:
Q1: What auth library is imported?
A1: passport-oauth2 (verified: auth.ts:3)
    Confidence: 100%

Q2: Where is OAuth config defined?
A2: config/oauth.json (verified: exists at project root)
    Confidence: 100%

Q3: What OAuth version is specified?
A3: version: "2.0" (verified: config/oauth.json:5)
    Confidence: 100%

STEP 4: Generate Final Verified Response
‚Üí Synthesize baseline + verification results
‚Üí Adjust baseline based on evidence
‚Üí Final confidence = average of verifications
‚Üí Mark any remaining assumptions

Final Response:
"The authentication system uses OAuth 2.0
‚úÖ Verified through 4-step CoVe process:
- passport-oauth2 library (auth.ts:3)
- OAuth config at config/oauth.json
- version: "2.0" specified (config/oauth.json:5)
Confidence: 100%"
```

**Enhanced .research.md Format:**

```markdown
## üîç META AI CoVe VERIFICATION

### Claim 1: [Statement to verify]

**Step 1 - Baseline**: [Initial assertion]

**Step 2 - Verification Questions**:
1. [Question 1]
2. [Question 2]
3. [Question 3]

**Step 3 - Execute Verifications**:
Q1: [Question 1]
A1: [Answer with source]
    Verified: [file:line]
    Confidence: X%

Q2: [Question 2]
A2: [Answer with source]
    Verified: [file:line]
    Confidence: Y%

Q3: [Question 3]
A3: [Answer with source]
    Verified: [file:line]
    Confidence: Z%

**Step 4 - Final Verified Response**:
[Synthesized answer incorporating all verifications]
Overall Confidence: [Average]%
Remaining Assumptions: [List if any]
```

**Performance Improvement:**
- Meta's research shows up to 23% improvement in reducing hallucinations
- Systematic questioning reveals gaps in initial understanding
- Forces explicit evidence gathering before claiming facts

---

### RULE 16: Microsoft's Magentic-One Task Ledger System

**Upgrade to RULE 7 (MAKER) with Microsoft Research's dual-loop planning**

**MANDATORY DUAL-LEDGER SYSTEM:**

```
OUTER LOOP - Task Ledger (Strategic):
‚Üí Maintains high-level plan
‚Üí Tracks facts vs. educated guesses
‚Üí Re-plans when progress stalls
‚Üí Updates strategy based on learnings

INNER LOOP - Progress Ledger (Tactical):
‚Üí Tracks current step execution
‚Üí Monitors completion status
‚Üí Detects when stuck
‚Üí Triggers outer loop replanning if needed
```

**Enhanced .plan.md Format:**

```markdown
## üìã TASK LEDGER (Strategic Plan)

### Facts (100% Confidence):
‚úÖ [Fact 1 - verified from code]
‚úÖ [Fact 2 - verified from code]
‚úÖ [Fact 3 - verified from docs]

### Educated Guesses (70-99% Confidence):
‚ö†Ô∏è [Guess 1 - based on patterns] - 85% confident
‚ö†Ô∏è [Guess 2 - inferred from structure] - 75% confident

### Overall Strategy:
[High-level approach to accomplish task]

### Replanning Triggers:
- If stuck for >2 steps ‚Üí Update Task Ledger
- If assumptions proven wrong ‚Üí Replan strategy
- If confidence drops <70% ‚Üí Re-evaluate approach

---

## üìä PROGRESS LEDGER (Tactical Execution)

### Step 1: [Action]
- Status: Not started
- Assigned to: [Tool/method]
- Expected outcome: [Result]
- Stall count: 0

### Step 2: [Action]
- Status: Not started
- Assigned to: [Tool/method]
- Expected outcome: [Result]
- Stall count: 0

[... continue for all steps ...]

### Progress Evaluation Points:
After EACH step, ask:
1. Is task complete? ‚Üí If yes, terminate
2. Is progress being made? ‚Üí If yes, continue
3. Stall count >2? ‚Üí If yes, update Task Ledger and replan
```

**Stall Detection & Recovery:**

```
WHEN implementing a step:

IF step succeeds:
  ‚Üí Mark complete
  ‚Üí Reset stall count to 0
  ‚Üí Move to next step

IF step fails or makes no progress:
  ‚Üí Increment stall count
  ‚Üí IF stall count ‚â§ 2:
    ‚Üí Try alternative approach
    ‚Üí Document attempt in Progress Ledger
  ‚Üí IF stall count > 2:
    ‚Üí STOP implementation
    ‚Üí Update Task Ledger with learnings
    ‚Üí Re-evaluate strategy
    ‚Üí Create new plan
    ‚Üí Reset stall count
    ‚Üí Request user approval for new plan
```

**Integration with RPI:**

```
Research Phase ‚Üí Populates Task Ledger with Facts
Planning Phase ‚Üí Creates initial Progress Ledger
Implementation Phase ‚Üí Updates Progress Ledger in real-time
Stall Detection ‚Üí Triggers replanning (back to Planning Phase)
```

---

### RULE 17: Stanford ACE Framework - Reflective Learning

**Add performance reflection to completion snapshots**

**MANDATORY REFLECTION PROCESS:**

```
AFTER completing EACH atomic step:

Execute Reflector Analysis:
1. What was attempted?
2. What was the outcome?
3. What worked well? (Helpful patterns)
4. What caused issues? (Harmful patterns)
5. What was neutral? (Contextual factors)
```

**Enhanced .completion-snapshot.md Format:**

```markdown
## üéì ACE REFLECTOR ANALYSIS

### Task Overview:
**What**: [Task description]
**Approach**: [Strategy used]
**Outcome**: [Success/Partial/Failure]
**Duration**: [Actual time vs. estimated]

---

### ‚úÖ HELPFUL PATTERNS (What Worked):

**Pattern 1**: [Specific technique that worked]
- **Context**: [When this was useful]
- **Evidence**: [Specific step where it helped]
- **Reusability**: [When to apply this again]
- **Confidence in pattern**: X%

**Pattern 2**: [Another successful approach]
- **Context**: [When this was useful]
- **Evidence**: [Specific step where it helped]
- **Reusability**: [When to apply this again]
- **Confidence in pattern**: Y%

---

### ‚ùå HARMFUL PATTERNS (What Failed):

**Anti-Pattern 1**: [Technique that didn't work]
- **Context**: [When this was tried]
- **Why it failed**: [Root cause analysis]
- **What to do instead**: [Alternative approach]
- **Lesson learned**: [Key takeaway]

**Anti-Pattern 2**: [Another failed approach]
- **Context**: [When this was tried]
- **Why it failed**: [Root cause analysis]
- **What to do instead**: [Alternative approach]
- **Lesson learned**: [Key takeaway]

---

### ‚óã NEUTRAL OBSERVATIONS (Contextual Factors):

**Observation 1**: [Environmental factor]
- **Impact**: [How it affected execution]
- **Future consideration**: [When to account for this]

**Observation 2**: [Technical constraint]
- **Impact**: [How it affected execution]
- **Future consideration**: [When to account for this]

---

### üìà PERFORMANCE METRICS:

**Efficiency**:
- Estimated time: [X] hours
- Actual time: [Y] hours
- Variance: [¬±Z%]
- Reason for variance: [Explanation]

**Accuracy**:
- Initial confidence: X%
- Final confidence: Y%
- Assumptions made: [N]
- Assumptions proven correct: [M]
- Accuracy rate: [M/N * 100]%

**Quality**:
- Tests run: [N]
- Tests passed: [M]
- Pass rate: [M/N * 100]%
- Bugs found during development: [count]
- Bugs fixed: [count]

---

### üéØ SKILL EXTRACTION (For Future Tasks):

**New Skills Developed**:
1. [Skill 1]: [Description and when to apply]
2. [Skill 2]: [Description and when to apply]

**Refined Skills**:
1. [Existing skill]: [How it was improved]
2. [Existing skill]: [How it was improved]

**Skills to Practice**:
1. [Skill needing improvement]: [Why and how to practice]

---

### üîÑ FEEDBACK LOOP:

**What would I do differently next time?**
1. [Improvement 1]
2. [Improvement 2]
3. [Improvement 3]

**What should be added to the framework?**
1. [Suggested enhancement to RPI/Operational rules]
2. [Suggested quality gate improvement]

**What assumptions should I avoid?**
1. [Assumption that caused issues]
2. [Better verification method to use instead]
```

**Automatic Skill Building:**

```
The Reflector component feeds a growing "skillbook":

~/.claude/skillbook.md

# RPI Framework Skillbook

## Authentication Tasks

### Helpful Patterns:
- Always read auth.ts before modifying login flow
- JWT config lives in config/auth.json (not .env)
- Test with invalid tokens, not just valid ones

### Harmful Patterns:
- Don't assume OAuth version from package name
- Don't skip reading session management code
- Don't test only happy path

## Database Tasks

### Helpful Patterns:
- Schema.ts is always source of truth
- Field name mismatches cause 80% of type errors
- Backend data shape !== UI assumptions

### Harmful Patterns:
- Never invent field names
- Never skip reading backend queries
- Never assume data structure from UI needs

[... grows over time with each task ...]
```

---

### RULE 18: DeepMind AlphaEvolve - Prompt Optimization

**Continuous improvement of verification and planning prompts**

**MANDATORY PROMPT TRACKING:**

```
Track effectiveness of prompts used in RPI workflow:

TCREI Clarification Prompts:
‚Üí Which questions most effectively clarify requirements?
‚Üí Which questions users answer most completely?
‚Üí Which questions reveal hidden complexity?

Verification Prompts (CoVe):
‚Üí Which verification questions catch most errors?
‚Üí Which evidence sources prove most reliable?
‚Üí Which confidence thresholds work best?

Planning Prompts (MAKER):
‚Üí Which decomposition strategies yield best results?
‚Üí Which atomic step sizes optimize for completion?
‚Üí Which estimation techniques prove most accurate?
```

**Enhanced Framework Evolution:**

```markdown
## üß¨ PROMPT OPTIMIZATION TRACKER

Location: ~/.claude/prompt-evolution.md

### TCREI Prompts - Effectiveness Tracking

**Most Effective Context Questions** (by success rate):
1. "What problem does this solve?" - 95% clarity improvement
2. "What's blocking you now?" - 88% reveals root cause
3. "What happens if we don't do this?" - 75% clarifies priority

**Least Effective Context Questions**:
1. "Why do you need this?" - 45% too vague
2. "What's the use case?" - 52% users give generic answers

**Optimization**: Replace low-performers with high-performers

---

### CoVe Verification Prompts - Accuracy Tracking

**High-Accuracy Verification Sources**:
1. Direct file reads (schema.ts, config files) - 98% accuracy
2. Grep for exact strings - 92% accuracy
3. Running actual commands - 95% accuracy

**Low-Accuracy Verification Sources**:
1. README files - 65% accuracy (often outdated)
2. Code comments - 58% accuracy (often wrong)
3. Package.json descriptions - 70% accuracy

**Optimization**: Prioritize high-accuracy sources in CoVe step 3

---

### MAKER Decomposition - Completion Rate Tracking

**Optimal Atomic Step Size**:
- <15 min steps: 95% completion rate, but too granular (overhead)
- 15-25 min steps: 98% completion rate ‚úÖ OPTIMAL
- 25-35 min steps: 85% completion rate (often split mid-execution)
- >35 min steps: 62% completion rate (get stuck)

**Optimization**: Target 20-minute atomic steps

---

### Confidence Calibration - Prediction Accuracy

**Confidence Score vs. Actual Success**:
90-100% confident ‚Üí 94% actually succeeded ‚úÖ Well calibrated
70-89% confident ‚Üí 78% actually succeeded ‚úÖ Well calibrated
50-69% confident ‚Üí 45% actually succeeded ‚ö†Ô∏è Overconfident
<50% confident ‚Üí 15% actually succeeded ‚úÖ Well calibrated

**Optimization**: Be more conservative with 50-69% range
```

**Automatic Prompt Evolution:**

```
After every 10 tasks, I analyze:

1. TCREI Questions:
   - Which got clear answers?
   - Which led to better research?
   - Update question templates

2. CoVe Sources:
   - Which sources were most accurate?
   - Which led to wrong conclusions?
   - Adjust verification priorities

3. MAKER Estimates:
   - Which tasks took longer than estimated?
   - Which categories need buffer time?
   - Refine estimation formulas

4. Confidence Scores:
   - Where was I overconfident?
   - Where was I underconfident?
   - Calibrate scoring rubric
```

---

## UPDATED COGNITIVE FRAMEWORK INTEGRATION

**Complete workflow with all 18 rules:**

```
PHASE 0: TCREI VALIDATION (Rules 6, 15, 18)
‚îú‚îÄ Verify all TCREI elements present
‚îú‚îÄ Use optimized prompts from Rule 18 tracker
‚îú‚îÄ If incomplete ‚Üí Ask refined clarification questions
‚îú‚îÄ Document TCREI in research phase
‚îî‚îÄ Confidence check: >70% to proceed

PHASE 1: RESEARCH (Rules 1, 8, 11, 12, 15)
‚îú‚îÄ Read schema.ts FIRST (Rule 11 - Schema is Law)
‚îú‚îÄ Read backend logic SECOND (Rule 12 - Context-First)
‚îú‚îÄ Launch sub-agents for heavy exploration (Rule 5)
‚îú‚îÄ Apply Meta's 4-Step CoVe (Rule 15):
‚îÇ  ‚îú‚îÄ Generate baseline assertions
‚îÇ  ‚îú‚îÄ Create verification questions
‚îÇ  ‚îú‚îÄ Execute verifications with file:line
‚îÇ  ‚îú‚îÄ Synthesize verified findings
‚îú‚îÄ Use high-accuracy sources (Rule 18 optimization)
‚îú‚îÄ Create .research.md with CoVe verification sections
‚îú‚îÄ STOP and present for approval
‚îî‚îÄ If confidence <80% ‚Üí Ask questions

PHASE 2: PLANNING (Rules 2, 7, 14, 16, 18)
‚îú‚îÄ Create Task Ledger (Rule 16 - Magentic-One):
‚îÇ  ‚îú‚îÄ Document Facts (100% confidence)
‚îÇ  ‚îú‚îÄ Document Educated Guesses (70-99%)
‚îÇ  ‚îú‚îÄ Define overall strategy
‚îÇ  ‚îî‚îÄ Set replanning triggers
‚îú‚îÄ Create Progress Ledger (Rule 16):
‚îÇ  ‚îú‚îÄ Apply MAKER decomposition (Rule 7)
‚îÇ  ‚îú‚îÄ Target 20-min atomic steps (Rule 18 optimization)
‚îÇ  ‚îú‚îÄ Specify standard tools (Rule 14)
‚îÇ  ‚îú‚îÄ Define test commands with exit codes
‚îÇ  ‚îî‚îÄ Set stall detection thresholds
‚îú‚îÄ Include confidence assessment
‚îú‚îÄ Create .plan.md with dual ledgers
‚îú‚îÄ STOP and present for approval
‚îî‚îÄ Only proceed if confidence >70%

PHASE 3: IMPLEMENTATION (Rules 3, 10, 13, 16, 17)
‚îú‚îÄ Execute atomic steps from Progress Ledger
‚îú‚îÄ After EACH step:
‚îÇ  ‚îú‚îÄ Run actual test command (Rule 10 - Anti-Vibe)
‚îÇ  ‚îú‚îÄ Verify exit code = 0
‚îÇ  ‚îú‚îÄ Document ACTUAL output
‚îÇ  ‚îú‚îÄ Execute ACE Reflector (Rule 17):
‚îÇ  ‚îÇ  ‚îú‚îÄ What worked? (Helpful patterns)
‚îÇ  ‚îÇ  ‚îú‚îÄ What failed? (Harmful patterns)
‚îÇ  ‚îÇ  ‚îú‚îÄ What was neutral? (Context)
‚îÇ  ‚îÇ  ‚îî‚îÄ Extract skills for skillbook
‚îÇ  ‚îú‚îÄ Update Progress Ledger
‚îÇ  ‚îú‚îÄ Detect stalls (Rule 16):
‚îÇ  ‚îÇ  ‚îú‚îÄ IF stall count >2 ‚Üí Update Task Ledger
‚îÇ  ‚îÇ  ‚îú‚îÄ Replan strategy
‚îÇ  ‚îÇ  ‚îî‚îÄ Get new approval
‚îÇ  ‚îú‚îÄ IF error occurs (Rule 13 - Recursive Debugging):
‚îÇ  ‚îÇ  ‚îú‚îÄ Analyze autonomously
‚îÇ  ‚îÇ  ‚îú‚îÄ IF fix confidence ‚â•70% ‚Üí Fix and re-test
‚îÇ  ‚îÇ  ‚îî‚îÄ IF fix confidence <70% ‚Üí Show analysis, ask
‚îÇ  ‚îî‚îÄ Mark complete ONLY after verification
‚îú‚îÄ Report progress with verified results
‚îú‚îÄ Monitor context window
‚îî‚îÄ Create .completion-snapshot.md with ACE Reflection

PHASE 4: LEARNING (Rules 17, 18)
‚îú‚îÄ Review ACE Reflector analysis
‚îú‚îÄ Update skillbook with patterns learned
‚îú‚îÄ Track prompt effectiveness
‚îú‚îÄ Calibrate confidence scoring
‚îú‚îÄ Update optimization tracker
‚îî‚îÄ Evolve framework based on evidence
```

---

## RESEARCH CREDITS & ACKNOWLEDGMENTS

**This enhanced framework integrates cutting-edge research from:**

### üî¨ Meta AI Research
- **Chain of Verification (CoVe)** - 4-step verification methodology
- **Paper**: "Chain-of-Verification Reduces Hallucination in Large Language Models"
- **Improvement**: Up to 23% reduction in hallucinations
- **Implementation**: Rule 15

### üß† Microsoft Research
- **Magentic-One** - Multi-agent task decomposition system
- **Innovation**: Dual-loop planning (Task Ledger + Progress Ledger)
- **Stall Detection**: Automatic replanning when stuck
- **Implementation**: Rule 16

### üéì Stanford University / SambaNova Research
- **Agentic Context Engine (ACE)** - Reflective learning framework
- **Innovation**: Agent + Reflector + SkillManager architecture
- **Learning**: Systematic skill extraction from experience
- **Implementation**: Rule 17

### ü§ñ Google DeepMind
- **AlphaEvolve** - Prompt optimization and evolution
- **Innovation**: Systematic tracking and improvement of prompts
- **Verification Code**: Evidence-based prompt refinement
- **Implementation**: Rule 18

### üìö Original Framework Foundations
- **Google AI Course (YouTube)**: TCREI Framework, CoVe foundations
- **AI Engineering Course (YouTube)**: MAKER Logic, Confidence Scoring
- **Anthropic Research**: Claude Code best practices
- **Academic Research**: Chain-of-Verification techniques

---

## COMPLETE RULE SUMMARY

**18 Mandatory Rules for Maximum Effectiveness:**

### Core RPI Workflow (Rules 1-5):
1. Research before implementation
2. Explicit planning with approval gates
3. Implement only after approval
4. Context window monitoring
5. Sub-agent discipline

### Cognitive Frameworks (Rules 6-9):
6. TCREI Input Validation
7. MAKER Task Decomposition
8. Chain of Verification (CoVe)
9. Confidence Scoring

### Operational Excellence (Rules 10-14):
10. Anti-Vibe Verification (execution = completion)
11. Schema is Law (data structure authority)
12. Context-First Pattern (backend before frontend)
13. Recursive Debugging Loop (autonomous recovery)
14. In-Distribution Tooling (standard over custom)

### Research-Backed Enhancements (Rules 15-18):
15. Meta's 4-Step CoVe (systematic verification)
16. Microsoft's Dual Ledgers (strategic + tactical planning)
17. Stanford's ACE Reflector (performance learning)
18. DeepMind's Prompt Evolution (continuous optimization)

---

**Enhanced RPI Framework v2.0: Research-backed rigor, zero hallucinations, autonomous excellence, continuous learning.**
